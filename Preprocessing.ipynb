{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d083695f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn==0.12.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.12.0)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from imbalanced-learn==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from imbalanced-learn==0.12.0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from imbalanced-learn==0.12.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from imbalanced-learn==0.12.0) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn==0.12.0 scikit-learn==1.3.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "qAtAVwJh0kmz",
      "metadata": {
        "id": "qAtAVwJh0kmz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.makedirs(\"outputs\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "KVaKBH8k0m9K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVaKBH8k0m9K",
        "outputId": "20c453dd-34f2-4d0d-c09e-09987e4a5cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING AND MERGING DATA\n",
            "============================================================\n",
            "Transaction shape: (590540, 394)\n",
            "Identity shape: (144233, 41)\n",
            " Merged dataset shape: (590540, 434)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"LOADING AND MERGING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "DATA_PATH = \"Data/\"  # Update if needed\n",
        "\n",
        "train_transaction = pd.read_csv(os.path.join(DATA_PATH, \"train_transaction.csv\"))\n",
        "train_identity = pd.read_csv(os.path.join(DATA_PATH, \"train_identity.csv\"))\n",
        "\n",
        "print(f\"Transaction shape: {train_transaction.shape}\")\n",
        "print(f\"Identity shape: {train_identity.shape}\")\n",
        "\n",
        "# Merge on TransactionID (left join - not all transactions have identity info)\n",
        "df = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
        "print(f\" Merged dataset shape: {df.shape}\")\n",
        "\n",
        "# Store original columns for reference\n",
        "original_cols = df.columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "QheV54ot074p",
      "metadata": {
        "id": "QheV54ot074p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " FEATURE ENGINEERING\n",
            "============================================================\n",
            "Created time-based features:\n",
            "   - Transaction_hour (0-23)\n",
            "   - Transaction_day_of_week (0-6)\n",
            "   - Transaction_day\n",
            "   - is_weekend (0/1)\n",
            "   - is_night (0/1)\n",
            "Created amount-based features:\n",
            "   - TransactionAmt_log\n",
            "   - TransactionAmt_decimal\n",
            "   - TransactionAmt_is_round\n",
            "Created email features:\n",
            "   - P_email_suffix, R_email_suffix\n",
            "   - email_match\n",
            " Created interaction features:\n",
            "   - card1_card2, addr1_addr2\n",
            "\n",
            " Total features after engineering: 447\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Time-based features from TransactionDT ---\n",
        "# TransactionDT is seconds from a reference point\n",
        "df['Transaction_hour'] = (df['TransactionDT'] // 3600) % 24\n",
        "df['Transaction_day_of_week'] = (df['TransactionDT'] // 86400) % 7\n",
        "df['Transaction_day'] = df['TransactionDT'] // 86400\n",
        "\n",
        "# Is weekend?\n",
        "df['is_weekend'] = (df['Transaction_day_of_week'] >= 5).astype(int)\n",
        "\n",
        "# Is night transaction (midnight to 6am)?\n",
        "df['is_night'] = ((df['Transaction_hour'] >= 0) & (df['Transaction_hour'] < 6)).astype(int)\n",
        "\n",
        "print(\"Created time-based features:\")\n",
        "print(\"   - Transaction_hour (0-23)\")\n",
        "print(\"   - Transaction_day_of_week (0-6)\")\n",
        "print(\"   - Transaction_day\")\n",
        "print(\"   - is_weekend (0/1)\")\n",
        "print(\"   - is_night (0/1)\")\n",
        "\n",
        "# Transaction Amount features\n",
        "df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])\n",
        "df['TransactionAmt_decimal'] = (df['TransactionAmt'] - df['TransactionAmt'].astype(int))\n",
        "df['TransactionAmt_is_round'] = (df['TransactionAmt_decimal'] == 0).astype(int)\n",
        "\n",
        "print(\"Created amount-based features:\")\n",
        "print(\"   - TransactionAmt_log\")\n",
        "print(\"   - TransactionAmt_decimal\")\n",
        "print(\"   - TransactionAmt_is_round\")\n",
        "\n",
        "# Email domain features\n",
        "def get_email_suffix(email):\n",
        "    if pd.isna(email):\n",
        "        return 'missing'\n",
        "    if '.com' in str(email):\n",
        "        return 'com'\n",
        "    elif '.net' in str(email):\n",
        "        return 'net'\n",
        "    elif '.org' in str(email):\n",
        "        return 'org'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "df['P_email_suffix'] = df['P_emaildomain'].apply(get_email_suffix)\n",
        "df['R_email_suffix'] = df['R_emaildomain'].apply(get_email_suffix)\n",
        "df['email_match'] = (df['P_emaildomain'] == df['R_emaildomain']).astype(int)\n",
        "\n",
        "print(\"Created email features:\")\n",
        "print(\"   - P_email_suffix, R_email_suffix\")\n",
        "print(\"   - email_match\")\n",
        "\n",
        "# Card features\n",
        "df['card1_card2'] = df['card1'].astype(str) + '_' + df['card2'].astype(str)\n",
        "df['addr1_addr2'] = df['addr1'].astype(str) + '_' + df['addr2'].astype(str)\n",
        "\n",
        "print(\" Created interaction features:\")\n",
        "print(\"   - card1_card2, addr1_addr2\")\n",
        "\n",
        "print(f\"\\n Total features after engineering: {df.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4h-Xf5eb1AYD",
      "metadata": {
        "id": "4h-Xf5eb1AYD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MISSING VALUE ANALYSIS\n",
            "============================================================\n",
            "Features with >90% missing: 12\n",
            "Features with >50% missing: 214\n",
            "Features with >0% missing: 414\n",
            "Features with 0% missing: 33\n",
            " Saved: outputs/missing_analysis.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MISSING VALUE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "missing_pct = df.isnull().mean().sort_values(ascending=False)\n",
        "missing_summary = pd.DataFrame({\n",
        "    'missing_count': df.isnull().sum(),\n",
        "    'missing_percent': missing_pct * 100\n",
        "}).sort_values('missing_percent', ascending=False)\n",
        "\n",
        "print(f\"Features with >90% missing: {(missing_pct > 0.9).sum()}\")\n",
        "print(f\"Features with >50% missing: {(missing_pct > 0.5).sum()}\")\n",
        "print(f\"Features with >0% missing: {(missing_pct > 0).sum()}\")\n",
        "print(f\"Features with 0% missing: {(missing_pct == 0).sum()}\")\n",
        "\n",
        "# Save missing summary\n",
        "missing_summary.to_csv(\"outputs/missing_analysis.csv\")\n",
        "print(\" Saved: outputs/missing_analysis.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "qsCvroa91DF6",
      "metadata": {
        "id": "qsCvroa91DF6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "REMOVING HIGH-MISSING COLUMNS (>80% missing)\n",
            "============================================================\n",
            "Dropping 74 columns with >80.0% missing values\n",
            " Shape after dropping high-missing columns: (590540, 373)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"REMOVING HIGH-MISSING COLUMNS (>80% missing)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "MISSING_THRESHOLD = 0.80\n",
        "cols_to_drop = missing_pct[missing_pct > MISSING_THRESHOLD].index.tolist()\n",
        "\n",
        "# Keep TransactionID and isFraud regardless\n",
        "cols_to_drop = [c for c in cols_to_drop if c not in ['TransactionID', 'isFraud']]\n",
        "\n",
        "print(f\"Dropping {len(cols_to_drop)} columns with >{MISSING_THRESHOLD*100}% missing values\")\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "print(f\" Shape after dropping high-missing columns: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Ku7vzUgm1Hug",
      "metadata": {
        "id": "Ku7vzUgm1Hug"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "5SEPARATING NUMERIC AND CATEGORICAL FEATURES\n",
            "============================================================\n",
            "Numeric features: 341\n",
            "Categorical features: 30\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5SEPARATING NUMERIC AND CATEGORICAL FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identify column types\n",
        "exclude_cols = ['TransactionID', 'isFraud']\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features: {len(numeric_cols)}\")\n",
        "print(f\"Categorical features: {len(categorical_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "NPFABHvZ1M6_",
      "metadata": {
        "id": "NPFABHvZ1M6_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " HANDLING MISSING VALUES\n",
            "============================================================\n",
            "Imputing numeric columns with median...\n",
            "Imputed 341 numeric columns\n",
            "Imputing categorical columns with 'Unknown'...\n",
            "Imputed 30 categorical columns\n",
            "\n",
            " Remaining missing values: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" HANDLING MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Numeric: Impute with median ---\n",
        "print(\"Imputing numeric columns with median...\")\n",
        "numeric_imputer = SimpleImputer(strategy='median')\n",
        "df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n",
        "print(f\"Imputed {len(numeric_cols)} numeric columns\")\n",
        "\n",
        "# --- Categorical: Impute with 'Unknown' ---\n",
        "print(\"Imputing categorical columns with 'Unknown'...\")\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna('Unknown')\n",
        "print(f\"Imputed {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df.isnull().sum().sum()\n",
        "print(f\"\\n Remaining missing values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "wz44GgDt1PeQ",
      "metadata": {
        "id": "wz44GgDt1PeQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "7ENCODING CATEGORICAL VARIABLES\n",
            "============================================================\n",
            "Low-cardinality columns (One-Hot): 24\n",
            "High-cardinality columns (Label Encoding): 6\n",
            "Label encoded 6 columns\n",
            "One-hot encoded 24 columns\n",
            "\n",
            " Shape after encoding: (590540, 406)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"7ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use Label Encoding for high-cardinality columns\n",
        "# Use One-Hot for low-cardinality columns\n",
        "LOW_CARDINALITY_THRESHOLD = 10\n",
        "\n",
        "low_card_cols = [c for c in categorical_cols if df[c].nunique() <= LOW_CARDINALITY_THRESHOLD]\n",
        "high_card_cols = [c for c in categorical_cols if df[c].nunique() > LOW_CARDINALITY_THRESHOLD]\n",
        "\n",
        "print(f\"Low-cardinality columns (One-Hot): {len(low_card_cols)}\")\n",
        "print(f\"High-cardinality columns (Label Encoding): {len(high_card_cols)}\")\n",
        "\n",
        "# Label encode high-cardinality columns\n",
        "label_encoders = {}\n",
        "for col in high_card_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "print(f\"Label encoded {len(high_card_cols)} columns\")\n",
        "\n",
        "# One-hot encode low-cardinality columns\n",
        "df = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
        "print(f\"One-hot encoded {len(low_card_cols)} columns\")\n",
        "\n",
        "print(f\"\\n Shape after encoding: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "CPGqgfRM1Re0",
      "metadata": {
        "id": "CPGqgfRM1Re0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " TRAIN/TEST SPLIT\n",
            "============================================================\n",
            "Training set: 472432 samples\n",
            "Test set: 118108 samples\n",
            "\n",
            "Training fraud rate: 3.50%\n",
            "Test fraud rate: 3.50%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['TransactionID', 'isFraud'])\n",
        "y = df['isFraud']\n",
        "\n",
        "# Stratified split to maintain fraud ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining fraud rate: {y_train.mean()*100:.2f}%\")\n",
        "print(f\"Test fraud rate: {y_test.mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Gi3OEjhS1TmT",
      "metadata": {
        "id": "Gi3OEjhS1TmT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "HANDLING CLASS IMBALANCE WITH SMOTE\n",
            "============================================================\n",
            "Before SMOTE:\n",
            "  Non-Fraud: 455902\n",
            "  Fraud: 16530\n",
            "  Ratio: 27.6:1\n",
            "\n",
            "After SMOTE:\n",
            "  Non-Fraud: 455902\n",
            "  Fraud: 227951\n",
            "  Ratio: 2.0:1\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"Before SMOTE:\")\n",
        "print(f\"  Non-Fraud: {(y_train == 0).sum()}\")\n",
        "print(f\"  Fraud: {(y_train == 1).sum()}\")\n",
        "print(f\"  Ratio: {(y_train == 0).sum() / (y_train == 1).sum():.1f}:1\")\n",
        "\n",
        "# Apply SMOTE only to training data\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # 1:2 ratio fraud:non-fraud\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"\\nAfter SMOTE:\")\n",
        "print(f\"  Non-Fraud: {(y_train_balanced == 0).sum()}\")\n",
        "print(f\"  Fraud: {(y_train_balanced == 1).sum()}\")\n",
        "print(f\"  Ratio: {(y_train_balanced == 0).sum() / (y_train_balanced == 1).sum():.1f}:1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2J5zFh4H1WBt",
      "metadata": {
        "id": "2J5zFh4H1WBt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE SCALING\n",
            "============================================================\n",
            "Applied StandardScaler\n",
            "   Training mean (sample): 0.0000\n",
            "   Training std (sample): 1.0000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data, transform both\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train_balanced),\n",
        "    columns=X_train_balanced.columns\n",
        ")\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    scaler.transform(X_test),\n",
        "    columns=X_test.columns\n",
        ")\n",
        "\n",
        "print(\"Applied StandardScaler\")\n",
        "print(f\"   Training mean (sample): {X_train_scaled.iloc[:, 0].mean():.4f}\")\n",
        "print(f\"   Training std (sample): {X_train_scaled.iloc[:, 0].std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f9PqerZH1Y4I",
      "metadata": {
        "id": "f9PqerZH1Y4I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " SAVING PROCESSED DATA\n",
            "============================================================\n",
            "Saved files:\n",
            "   - outputs/X_train_scaled.csv (SMOTE + scaled)\n",
            "   - outputs/X_test_scaled.csv (scaled)\n",
            "   - outputs/X_train_balanced.csv (SMOTE, unscaled)\n",
            "   - outputs/X_test.csv (unscaled)\n",
            "   - outputs/y_train_balanced.csv\n",
            "   - outputs/y_test.csv\n",
            "   - outputs/X_train_original.csv (no SMOTE)\n",
            "   - outputs/y_train_original.csv\n",
            "   - outputs/feature_names.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" SAVING PROCESSED DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save scaled data (for models sensitive to scale like Logistic Regression, SVM)\n",
        "X_train_scaled.to_csv(\"outputs/X_train_scaled.csv\", index=False)\n",
        "X_test_scaled.to_csv(\"outputs/X_test_scaled.csv\", index=False)\n",
        "\n",
        "# Save unscaled balanced data (for tree-based models)\n",
        "X_train_balanced.to_csv(\"outputs/X_train_balanced.csv\", index=False)\n",
        "X_test.to_csv(\"outputs/X_test.csv\", index=False)\n",
        "\n",
        "# Save labels\n",
        "y_train_balanced.to_csv(\"outputs/y_train_balanced.csv\", index=False)\n",
        "y_test.to_csv(\"outputs/y_test.csv\", index=False)\n",
        "\n",
        "# Save original (unbalanced) training data too\n",
        "X_train.to_csv(\"outputs/X_train_original.csv\", index=False)\n",
        "y_train.to_csv(\"outputs/y_train_original.csv\", index=False)\n",
        "\n",
        "# Save feature names\n",
        "feature_names = pd.DataFrame({'feature': X_train.columns.tolist()})\n",
        "feature_names.to_csv(\"outputs/feature_names.csv\", index=False)\n",
        "\n",
        "print(\"Saved files:\")\n",
        "print(\"   - outputs/X_train_scaled.csv (SMOTE + scaled)\")\n",
        "print(\"   - outputs/X_test_scaled.csv (scaled)\")\n",
        "print(\"   - outputs/X_train_balanced.csv (SMOTE, unscaled)\")\n",
        "print(\"   - outputs/X_test.csv (unscaled)\")\n",
        "print(\"   - outputs/y_train_balanced.csv\")\n",
        "print(\"   - outputs/y_test.csv\")\n",
        "print(\"   - outputs/X_train_original.csv (no SMOTE)\")\n",
        "print(\"   - outputs/y_train_original.csv\")\n",
        "print(\"   - outputs/feature_names.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Jyom9Y1F1c2k",
      "metadata": {
        "id": "Jyom9Y1F1c2k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREPROCESSING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Dataset Overview:\n",
            "-----------------\n",
            "Original features: 434\n",
            "Final features: 404\n",
            "Features dropped (high missing): 74\n",
            "\n",
            "Samples:\n",
            "--------\n",
            "Total samples: 590540\n",
            "Training samples (original): 472432\n",
            "Training samples (after SMOTE): 683853\n",
            "Test samples: 118108\n",
            "\n",
            "Class Distribution:\n",
            "-------------------\n",
            "Original fraud rate: 3.50%\n",
            "Training fraud rate (after SMOTE): 33.33%\n",
            "Test fraud rate: 3.50%\n",
            "\n",
            "Feature Engineering:\n",
            "--------------------\n",
            "- Time features: Transaction_hour, Transaction_day_of_week, is_weekend, is_night\n",
            "- Amount features: TransactionAmt_log, TransactionAmt_decimal, TransactionAmt_is_round\n",
            "- Email features: P_email_suffix, R_email_suffix, email_match\n",
            "- Interaction features: card1_card2, addr1_addr2\n",
            "\n",
            "Processing Steps:\n",
            "-----------------\n",
            "1.  Merged transaction + identity data\n",
            "2.  Created 8 engineered features\n",
            "3.  Removed 74 high-missing columns (>80%)\n",
            "4.  Imputed numeric (median) and categorical (Unknown)\n",
            "5.  Label encoded high-cardinality categoricals\n",
            "6.  One-hot encoded low-cardinality categoricals\n",
            "7.  Train/test split (80/20, stratified)\n",
            "8.  Applied SMOTE for class balancing\n",
            "9.  Scaled features with StandardScaler\n",
            "\n",
            "Saved: outputs/preprocessing_summary.txt\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = f\"\"\"\n",
        "Dataset Overview:\n",
        "-----------------\n",
        "Original features: 434\n",
        "Final features: {X_train.shape[1]}\n",
        "Features dropped (high missing): {len(cols_to_drop)}\n",
        "\n",
        "Samples:\n",
        "--------\n",
        "Total samples: {len(df)}\n",
        "Training samples (original): {len(X_train)}\n",
        "Training samples (after SMOTE): {len(X_train_balanced)}\n",
        "Test samples: {len(X_test)}\n",
        "\n",
        "Class Distribution:\n",
        "-------------------\n",
        "Original fraud rate: 3.50%\n",
        "Training fraud rate (after SMOTE): {y_train_balanced.mean()*100:.2f}%\n",
        "Test fraud rate: {y_test.mean()*100:.2f}%\n",
        "\n",
        "Feature Engineering:\n",
        "--------------------\n",
        "- Time features: Transaction_hour, Transaction_day_of_week, is_weekend, is_night\n",
        "- Amount features: TransactionAmt_log, TransactionAmt_decimal, TransactionAmt_is_round\n",
        "- Email features: P_email_suffix, R_email_suffix, email_match\n",
        "- Interaction features: card1_card2, addr1_addr2\n",
        "\n",
        "Processing Steps:\n",
        "-----------------\n",
        "1.  Merged transaction + identity data\n",
        "2.  Created {8} engineered features\n",
        "3.  Removed {len(cols_to_drop)} high-missing columns (>80%)\n",
        "4.  Imputed numeric (median) and categorical (Unknown)\n",
        "5.  Label encoded high-cardinality categoricals\n",
        "6.  One-hot encoded low-cardinality categoricals\n",
        "7.  Train/test split (80/20, stratified)\n",
        "8.  Applied SMOTE for class balancing\n",
        "9.  Scaled features with StandardScaler\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary\n",
        "with open(\"outputs/preprocessing_summary.txt\", \"w\") as f:\n",
        "    f.write(summary)\n",
        "print(\"Saved: outputs/preprocessing_summary.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
